운영체제가 하는일 개요

1. 프로세스 관리

인터넷 브라우저를 켜놓고 노래를 들으면서 게임을 하고 있어도 전부 동시에 실행된다.
게임을 하고 있으면 키보드와 마우스는 게임에만 반응을 하고 다른 프로그램은 백그라운드에서 실행되고 있어야 한다.
만약 운영체제가 관리를 하지 않는다면, 브라우저가 CPU를 독차지해서 게임이 실행되지 않거나, 
게임이 CPU 를 독차지 해서 노래가 실행되지 않을 수 있다.


2. 메모리 관리

모든 프로그램은 메모리에 올라와서 동작한다.
오늘날 운영체제는 여러 프로그램을 동시에 실행시키기 때문에
여러프로그램을 메모리에서 관리하는 방법을 알아보자.


3. 하드웨어 관리

운영체제는 사용자의 하드웨어에 대한 직접적인 접근을 막는다.
하드디스크에 데이터를 저장할 때, 하드 디스크의 특정 영역에 바로 저장하지 못하게 하고 
운영체자가 판단하여 적절한 위치에 저장한다.
그 이유는 하드디스크의 특정 영역에 다른 중요한 데이터가 있을 수 있고, 사용자가 악의적으로 공격할 수 있기 때문이다.



4. 파일 시스템 관리

하드 디스크에 많은 파일들의 효율적인 저장과 관리를 하기 위한 방법을 알아보게 된다.




[운영체제의 구조]

운영체제의 핵심은 커널이다.

커널은 프로세스와 메모리, 저장장치를 관리하는 핵심적인 기능을 담당한다.

사용자는 운영 체제의 커널에 직접 접근할 수 없고 인터페이스를 통해서 접근이 가능하다.

인터페이스는 GUI 와 CLI 두개로 나눌 수 있는데,

텍스트냐 그림이냐의 차이만 있을 뿐 커널에 접근하기 위한 목적은 같다.


1) GUI (Graphic User Interface)

그래픽으로 된 인터페이스이다.

window, mac os 와 같이 그래픽으로 커널과 상호 작용하기 때문에,

일반 사용자도 사용하기가 쉽다.



2) CLI (Command-Line Interface)

unix 나 linux 와 같은 운영체제가 기본적으로 제공하는 인터페이스로 텍스트를 이용해 커널과 상호작용 한다.




어플리케이션은 시스템 콜을 통해서 커널에 접근할 수 있다.

커널은 사용자로부터 자신을 보호하기 위한 "System Call" 이라는 인터페이스를 가지고 있다.

사용자나 어플리케이션이 하드디스크에 데이터를 저장한다고 해보자.

시스템 콜이 없이 어플리케이션이 하드디스크에 직접 접근하면, 중요한 데이터를 덮어쓸 수도 있고 

해당 사용자나 어플리케이션이 저장한 데이터를 다른 어플리케이션이 덮어 쓸 수도 있다.

시스템 콜을 이용하면 커널에서 제공하는 Write 함수를 사용하게 되는데, 

그럼 운영체제가 알아서 하드디스크의 빈 공간에 저장하게 된다.



사용자와 애플리케이션은 커널과의 인터페이스로 시스템 콜을 사용한다고 했는데,

하드웨어와 커널의 인터페이스로는 "드라이버" 를 사용한다.

운영체제는 많은 종류의 하드웨어를 전부 지원해야 하기 때문에,

각각의 하드웨어에 맞는 프로그램을 커널이 미리 전부 가지고 있기는 어렵다.

그리하여 하드웨어를 제작한 제조사에서, 드라이버를 만들어서 제공하는 것이 일반적인 방법이다.

보통 키보드나 마우스 같은 간단한 장치들은 커널에 포함되어 있어, 컴퓨터에 꽂으면 바로 동작하지만 

그래픽카드와 같은 복잡한 장치들은 디바이스 드라이버를 설치해서 사용해야 한다.





[컴퓨터 하드웨어와 구조]

현대의 대부분 컴퓨터들은 프로그램 내장방식의 "폰 노이만 구조" 를 채택하고 있다.

폰노이만 구조는 폰노이만이라는 수학자가 고안해서 만든것이라 폰 노이만 구조라고 불린다.

고전 컴퓨터인 애니악에서는 하드웨어로 프로그램을 만들었기 때문에 프로그램이 달라질때마다 매번 스위치와 배선을 다시 조정해야 했다.

폰 노이만은 이를 해결하기 위해서 CPU 와 메모리를 두고 둘 사이는 버스로 연결하는 방식으로 컴퓨터를 새로 개발했다.

버스는 데이터를 전달하는 통로를 말한다.


폰 노이만 구조를 채택한 컴퓨터에서 프로그램은 메모리에 올려져서 실행되는데, 

프로그램을 메모리에 내장했다고 해서 프로그램 내장방식이라고 부른다.

메모리에 올라간 프로그램들은 명령에 따라 처리되고, 애니악에서 프로그래밍 할 때 처럼 하드웨어 배선을 바꾸는 대신 소프트웨어만 바꿔주면 되기 때문에 매우 편린해졌다.


1) 메인보드의 구조

메인보드는 다른 하드웨어를 연결하는 장치이다.

장치간에 데이터를 전송하는 건 메인보드의 버스가 담당한다.

폰 노이만 구조이므로 CPU 와 메모리가 필수이다.



2) CPU 의 구조

CPU 는 Central Processing Unit 의 약자로 중앙처리 장치라는 뜻이다.

CPU 를 구성하는 장치는 세 가지고 나뉜다.

산술 논리 연산장치 (ALU)
ALU는 CPU의 심장부라고 할 수 있으며, 모든 산술(덧셈, 뺄셈, 곱셈, 나눗셈 등)과 논리(AND, OR, NOT, XOR 등) 연산을 담당합니다. 프로그램이 실행되는 동안, ALU는 이러한 연산을 통해 데이터를 처리하고, 그 결과를 다시 메모리나 다른 하드웨어로 전송합니다.

산술 연산: 숫자 데이터에 대한 수학적 연산을 수행합니다.
논리 연산: 불리언 논리를 기반으로 데이터 비트에 대한 연산을 수행합니다.
비교 연산: 데이터 값을 비교하고 조건에 따른 분기를 결정합니다.
제어장치 (Control Unit, CU)
제어장치는 CPU의 또 다른 핵심 부분으로, 모든 CPU 활동을 조정하고 관리합니다. 제어장치는 프로그램의 명령어를 해석하고, 그에 따라 CPU 내의 다른 부분들이나 연결된 하드웨어 장치들이 조화롭게 작동할 수 있도록 명령을 내립니다.

명령어 해석: 저장된 명령어를 해석하여 어떤 작업을 해야 할지 결정합니다.
명령어 실행: 해석된 명령어에 따라 ALU, 레지스터 등에 구체적인 작업을 지시합니다.
시퀀싱: 명령어의 실행 순서를 관리하고, 필요한 경우 명령어 사이의 순서를 조정합니다.
레지스터
레지스터는 CPU 내부에 위치한 매우 빠른 기억 장치로, 처리 중인 데이터와 명령어를 일시적으로 저장합니다. 레지스터는 데이터 접근 속도가 매우 빠르기 때문에, CPU가 자주 사용하는 데이터나 중간 계산 결과를 저장하는 데 사용됩니다.

데이터 레지스터: 산술 및 논리 연산에 사용되는 데이터를 저장합니다.
주소 레지스터: 메모리 주소를 저장하여, 메모리 관리에 사용됩니다.
상태 레지스터: CPU의 상태를 나타내는 플래그(예: 오버플로, 제로, 캐리 플래그 등)를 저장합니다.
명령 레지스터: 현재 실행 중인 명령어를 저장합니다.



3) 메모리의 종류

메모리는 크게 RAM 과 ROM 으로 구별할 수 있다.

1.RAM (Random Access Memory)
램은 랜덤으로 데이터를 읽어도 저장된 위치와 상관 없이 읽는 속도가 같다.
램은 전력이 끊기면 데이터를 모두 잃어버리기 때문에 메인 메모리로 사용한다.


2.ROM (Read Only Memory)
Rom 은 전력이 끝겨도 데이터를 계속 보관할 수 있지만,
데이터를 한 번 쓰면 수정이 불가능하다. 
그래서 Rom 은 컴퓨터의 부팅과 관련된 바이오스를 저장하는 데에 주로 사용된다.
현대의 컴퓨터 시스템에서 ROM이라는 용어는 기술적으로는 부정확할 수 있으며, 
실제로는 플래시 메모리 또는 EEPROM이 사용되는 경우가 많다. 
이러한 기술적 변화는 펌웨어의 유연성과 확장성을 크게 향상시켰다.





[컴퓨터의 부팅 과정]

컴퓨터의 전원을 누르면 Rom 에 저장된 바이오스가 실행된다.

바이오스는 전원, CPU, 메모리, 키보드, 마우스, 하드디스크 등 주요 하드웨어에 이상이 없는지 체크한다.

만약 주요장치에 이상이 있다면, 오류음을 내면서 부팅이 이뤄지지 않고 이상이 없다면 하드디스크에 있는 마스터 부트 레코드에 저장된 

부트로더를 메모리로 가져와서 실행한다.

만약 windows 운영체제와 리눅스 운영체제가 둘다 설치되어 있는 컴퓨터라면,

어떤 운영체제를 실행할지 선택하는 화면이 나온다.

운영체제를 선택했거나 운영체제가 하나면, 바로 운영체제를 메모리로 불러오고 이제 부터 실행되는 모든 응용 프로그램은 메모리에 올라와서 운영체제가 관리한다.





[인터럽트]

cpu 가 입출력 장치에 데이터를 읽거나 쓰려고 하는 상황을 생각해보자.

cpu 는 입출력 작업이 들어오면 입출력 관리자에게 입출력 명령을 내린다.

cpu 관점에서는 입출력 명령이 언제 완료될지 알 수 없기 때문에, 주기적으로 계속 확인해줘야 한다.

이러한 방식을 폴링 (Polling) 방식이라고 한다.

폴링방식의 단점은 주기적으로 CPU가 확인해줘야 하니까 성능이 좋지 않다는 것이다.

인터럽트는 폴링 방식의 단점을 해결한 방식이다.



cpu 가 입출력 관리자에게 입출력 명령을 내리고, 자기는 다른 작업을 계속 한다.

입출력 관리자는 입출력이 완료되었을 때, CPU에게 신호를 주고 CPU 는 그 신호를 받아 

인터럽트 서비스 루틴 (ISR)을 실행시켜 작업을 완료한다.

인터럽트 서비스 루틴은 특정 인터럽트가 들어오면 그 인터럽트를 처리하는 함수이다.

인터럽트는 비동기적으로 동작하기 때문에 성능에 이점이 있다.

인터럽트는 하드웨어 인터럽트와 소프트웨어 인터럽트 두 가지 방식이 존재한다.


하드웨어 방식은 입출력 등과 같은 인터럽트가 있고 

소프트웨어 방식은 사용자 프로그램에서 발생한 인터럽트가 있다.

예를들면 유효하지 않는 메모리에 접근하거나, 0 으로 나누는 명령어 등이 존재한다.



====== 

인터럽트는 컴퓨터 아키텍처에서 매우 중요한 개념으로, 

실행 중인 프로그램 외부 또는 내부에서 발생하는 이벤트에 대해 CPU가 반응하여 처리를 수행하도록 하는 메커니즘을 말한다. 

인터럽트를 통해 컴퓨터 시스템은 다양한 작업을 효율적으로 동시에 처리할 수 있다.


- 인터럽트의 종류

1) 하드웨어 인터럽트

외부 장치로부터의 신호에 의해 발생한다.

예를 들어, 키보드 입력, 네트워크 카드의 데이터 수신, 타이머 신호 등이 있다. 

이러한 인터럽트는 일반적으로 장치 컨트롤러가 CPU에 신호를 보내 인터럽트 요청을 전달한다.


2) 소프트웨어 인터럽트

프로그램 코드에 의해 의도적으로 발생되는 인터럽트다. 

예를 들어, 시스템 호출, 예외 처리(예: 0으로 나누기, 유효하지 않은 메모리 접근) 등이 이에 해당한다. 

소프트웨어 인터럽트는 프로그램이 실행 중에 특정 조건을 만족하거나 요청할 때 발생한다.


- 인터럽트 처리 과정

1) 인터럽트 요청(IRQ)

장치 또는 프로그램이 CPU에 작업을 요청할 때 인터럽트를 발생시킨다.

2) 인터럽트 서비스 루틴(ISR)

인터럽트가 발생하면, CPU는 현재 실행 중인 작업의 상태를 저장하고, 정의된 인터럽트 서비스 루틴으로 제어를 전환한다.
이 루틴은 인터럽트의 원인이 된 작업을 처리한다.

3) 작업 복귀

ISR 실행이 완료되면, CPU는 인터럽트 발생 전의 작업 상태를 복원하고 이전에 실행 중이던 작업으로 돌아간다.




[프로그램과 프로세스]

프로그램이란?

프로그램은 하드디스크와 같은 저장장치에 저장된 명령문의 집합체를 뜻한다.

애플리케이션이나 앱이라고도 불리고 windows 운영체제에서는 .exe 모습을 하고 있다.

프로그램은 컴퓨터 관점에서 하드디스크 즉, 저장 장치만 사용하는 수동적인 존재이다.


프로세스란?

실행중인 프로그램이라고 간단하게 표현할 수 있다.

실행중인 프로그램이란 하드디스크에 저장된 프로그램이 메모리에 올라갔을 때 

실행중인 프로그램, 즉 프로세스라고 불린다.

프로세스는 메모리도 사용하고 운영체제의 CPU 스케줄링 알고리즘에 따라서 CPU 도 사용하고 필요에 따라 입출력을 하기 때문에 능동적인 존재라고 말할 수 있다.



프로세스의 구조?

프로세스는 Code 영역, Data 영역, Stack 영역, Heap 영역이 존재한다.

Code 영역에는 자신을 실행하는 코드가 저장되어 있고 

Data 영역은 전역 변수와 Static 변수가 저장되어 있다.

Stack 영역에는 지역변수와 함수 호출을 했을 때 필요한 정보들이 저장된다.

Heap 영역은 프로그래머가 동적으로 메모리를 할당하는 데에 쓰이는 영역이다. c언어에서 malloc(), free() 함수를 호출하면 

Heap 영역에서 자원을 할당/해제 할 수 있다.



프로세스가 되는 과정

c 언어를 기준으로 설명

c 언어는 컴파일 언어이기 때문에 컴파일이 돼야 실행가능하므로 컴파일을 해줘야 한다.

컴파일 과정은 먼저 전처리기를 거쳐서 매크로로 정의한 숫자를 치환하고 필요한 파일을 불러온다.

전처리기를 거치면 파일의 확장자는 .i 가 된다.

그 다음 컴파일러가 컴파일을 해준다.

컴파일을 마치면 고수준인 C언어를 저수준 언어인 어셈블리어로 바꿔준다.

어셈블리어는 명령어가 기계어랑 일대일 매칭이 되기 때문에 기계어와 가장 가까운 언어라고 볼수 있다.

컴파일러를 거치면 파일의 확장자는 .s 가 된다.

이제 어셈블러가 어셈블리어를 기계어로 바꿔준다.

그럼 해당 파일은 0,1 로 이루어진 기계어로 구성되고 파일의 확장자는 .o 가 된다.

기계어로 구성되어 있기 때문에 파일을 텍스트 에디터로 열어보면 글씨가 깨져서 보인다.

마지막으로 링커가 링킹을 한다.

링킹이란 여러가지 라이브러리나 다른 소스코드를 연결하는 것이다.

이렇게 링킹까지 거치면 파일의 확장자는 .exe 가 된다.

이제 해당 .exe 파일을 더블클릭하게 되면 text.exe 라는 하드디스크에 존재하는 파일이 메모리에 올라가게 되고 

이렇게 올라간 프로그램은 프로세스라는 새로운 이름으로 불리게 된다.

해당 프로세스는 올라간 시점부터 운영체제에 의해 관리된다.

운영체제의 전략에 따라서 프로세스가 실행될 텐데 이를 cpu 관점에서 살펴보자.


cpu 는 0과 1 과 같은 기계어만을 실행하는데, 

가독성이 너무 떨어지기 때문에 어셈블리어로 살펴보자.

cpu 내의 제어장치가 숫자 5와 7을 메모리에 저장시킨다.

그리고 이 메모리에 저장된 값을 edx, eax 레지스터도 가져온다.
 
제어장치가 레지스터에 저장된 5,7 을 가지고 더하라는 명령을 하면 산술 논리 연산장치가 두 숫자를 더하고 

그 결과를 eax 레지스터에 저장한다. 다시 제어장치가 eax 레지스터에 저장된 12를 가져와서 메모리에 저장시킨다.





[멀티프로그래밍과 멀티프로세싱]

유니프로그래밍은 메모리에 오직 하나의 프로세스가 올라온 것을 말한다.

반대로 멀티프로그래밍은 메모리에 여러 개의 프로세스가 올라온 것을 말한다.

유니 프로그래밍과 멀티프로그래밍은 메모리의 관점으로 정의했다면,

멀티프로세싱은 CPU 관점으로 정의한 것이다.

멀티프로세싱은 CPU가 여러 개의 프로세스를 처리하는 것을 말한다.

오늘날의 OS 는 멀티프로그래밍과 멀티프로세싱 두 개가 공존한다.

메모리에는 여러 개의 프로세스가 올라오는 멀티프로그래밍이 있고,

시분할 처리로 CPU 가 각각의 프로세스를 짧은 시간 동안 교대로 실행하는 멀티프로세싱이 있다.

과거에는 메모리의 크기가 작아서 멀티프로그래밍이 불가능했다.

이때는 유니프로그래밍을 하면서 멀티프로세싱을 이용했다.

메모리에 프로세스를 올려서 CPU 로 처리를 하고 해당 프로세스를 다른 저장장치에 저장한다.

그리고 다른 저장장치에 있던 프로세스를 메모리에 올려서 CPU 로 처리하는 멀티프로세싱 기법을 사용했다.

이때 메모리에 있는 데이터를 다른 저장장치로 보내고 다른 저장장치에서 메모리에 올리는 것을 스와핑(Swapping) 이라고 한다.




[PCB]

프로그램이 메모리에 올라가서 실행중인 상태를 "프로세스" 라고 말한다.

운영체제는 여러개의 프로세스를 전부 다 관리하고 공평하게 실행시켜야 한다.

프로세스가 만들어지면 운영체제는 해당 프로세스의 정보를 가지고 있는 PCB(Process Control Block) 를 만들고 저장한다.

PCB 들은 연결리스트라는 자료구조로 저장이 된다.

연결리스트는 각각의 데이터가 다음 데이터를 연결하는 구조로 되어있는 자료구조 이다.

운영체제는 프로세스가 종료되면 연결리스트에서 해당 프로세스의 PCB 를 제거한다.


PCB 의 구조에서 

포인터는 부모와 자식 프로세스에 대한 포인터와 할당된 자원에 대한 포인터가 있고, 프로세스의 한 상태에서 다른 상태로 전환될 때 저장하는 포인터를 가지고 있다.

프로세스 상태는 현재 프로세스의 5가지 상태 - 생성, 준비, 실행, 대기, 완료 등을 나타낸다.

프로세스 ID 는 프로세스를 식별하기 위한 숫자가 저장된다.

프로그램 카운터는 다음에 실행될 명령어의 주소를 포함하는 프로그램 카운터를 저장한다.

오늘날 OS는 시분할처리로 여러 프로세스를 짧은 시간동안 번갈아 실행한다.

어떤 프로세스가 실행되다가 다른 프로세스에게 CPU 를 뺏기고 다시 실행될 때, 원래 실행하던 명령어가 실행되어야 하기 때문에

프로그램 카운터가 반드시 존재해야 한다.

레지스터 정보는 프로세스가 실행될 때 사용했던 레지스터 값들이 저장된다.

메모리 관련 정보에는 프로세스가 메모리에 있는 위치 정보, 메모리 침법을 막기 위한 경계 레지스터 값 등이 저장된다.

CPU 스케쥴링 정보에는 CPU 스케쥴링에 필요한 우선순위, 최종 실행시간, CPU 점유시간 등이 저장된다.




[프로세스 상태]

사용자가 프로그램을 실행시키면 메모리에 올라가면서 프로세스가 생성된다.

오늘날의 운영체제에는 동시에 수 많은 프로세스가 실행된다.

작업관리자를 확인해보면 수 많은 프로세스가 실행중인 것을 확인할 수 있다.

시분할 시스템을 사용하는 운영체제는 여러개의 프로세스를 돌아가면서 실행한다.

CPU 가 여러개의 프로세스를 동시에 실행한다는 의미가 아니라 한 순간에는 하나의 프로세스 밖에 처리하지 못한다는 것이다.

다만 속도가 매우 빨라서 사람이 보기에는 동시에 처리되는 것처럼 보인다.

프로세스는 시분할 처리를 위한 다섯가지 상태를 가지고 있다.

바로 생성,준비,실행,대기,완료 상태가 있다.

생성 상태에서는 PCB 를 생성하고 메모리에 프로그램 적재를 요청한 상태를 의미한다.

메모리에 프로그램 적재를 승인받으면 준비상태로 넘어간다.

준비상태는 CPU 를 사용하기 위해 기다리고 있는 상태이다.

준비상태에 있는 프로세스는 CPU 스케줄러에 의해 CPU 가 할당된다.

대부분의 프로세스는 준비상태에 존재한다.

실행상태는 준비상태에 있는 프로세스가 CPU 스케줄러에 의해 CPU 를 할당받아 실행되는 상태를 말한다.

실행상태에 있는 프로세스의 수는 CPU의 개수만큼이다.

CPU 가 한개라면 실행상태의 프로세스는 최대 한 개이다.

실행상태의 있는 프로세스도 CPU 를 무한정 쓸 수 있는 것이 아니고 부여된 시간만큼만 사용할 수 있다.

CPU 스케줄러는 부여된 시간을 초과하면 할당된 CPU 를 강제로 빼앗는다.

그러면 프로세스는 다시 준비상태로 돌아가게 된다.

대기 상태는 프로세스가 입출력 요청을 하면 입출력이 완료될 때 까지 기다리는 상태이다.

CPU 는 굉장히 빠른장치이지만 입출력 작업은 상당히 느린 작업이다.

특정 프로세스가 입출력 요청을 하면 입출력이 완료될 때까지 CPU 를 기다리게 하는건 굉장히 비효율적인 일이다.

그 대신에, 입출력 요청을 한 프로세스를 "대기상태" 로 두고 다른 프로세스에게 CPU 를 할당한다.

그러다가 시간이 지나서 입출력 작업이 완료되면 "대기상태" 에 있던 프로세스에게 CPU 할당 기회를 준다.

이런방식을 취하면 CPU 에게는 쉬는 시간을 주지않고 일을 시킬 수 있다.

마지막으로 완료상태는 프로세스가 종료된 상태를 의미한다.

프로세스가 사용했던 데이터를 메모리에서 제거하고 생성된 PCB도 제거해준다.




[context switching]

컨텍스트 스위칭은 프로세스를 실행하는 중에 다른 프로세스를 실행하기 위해 

실행중인 프로세스의 상태를 저장하고 다른 프로세스의 상태값으로 교체하는 작업이다.

컨텍스트 스위칭이 일어날 때 PCB 의 내용이 변경된다.

실행중인 프로세스의 작업 내용을 PCB에 저장하고 실행될 기존 프로세스의 PCB 의 내용대로 CPU가 다시 셋팅된다.

Context Switching 이 일어날때 PCB 에 변경하는 값들로는 

프로세스 상태, 다음 실행할 명령어의 주소를 담고 있는 프로그램 카운터,

각종 레지스터 값 등이 존재한다.

-- 상황

프로세스 A 가 실행을 하는데 CPU 점유 시간을 초과하였다.

운영체제는 프로세스A 기 CPU를 너무 오래 사용했다고 판단하고 인터럽트를 발생시킨다.

프로세스A 는 하던일을 멈춘다. 그리고 나중에 현재 상태에서 시작되어야 하기 때문에, 현재 CPU 의 레지스터 값 등을 PCB A에 저장한다.

이제 PCB B 를 참조해서 이전 프로세스 B의 상태로 CPU 의 레지스터 값을 설정한다.

여기에는 다음 실행할 명령어의 주소를 가지고 있는 프로그램 카운터(PC) 를 가지고 있기 때문에 

바로 프로세스 B의 명령어를 실행할 수 있다.

프로세스B가 점유시간동안 CPU 를 사용하다가 점유시간이 다되면 운영체제는 다시 인터럽트를 발생시킨다.

그리고 프로세스 B의 현재상태를 PCB B 에 저장하고 PCB A 에서 프로세스 A 의 ㅅ강태를 가져오고 다시 프로세스 A 를 실행시킨다.

이런식으로 메모리에 있는 모든 프로세스들은 컨텍스트 스위칭을 한다.

컨텍스트 스위칭을 하는 이유는 다양하다.

CPU 점유시간이 다 되거나, I/O 요청이 있거나 다른종류의 인터럽트가 있을 때 발생할 수 있다.





[프로세스의 생성과 종료]

일반적으로 프로세스가 생성될 때는 다음과 같은 방법으로 생성된다.

실행파일을 더블클릭으로 실행하면 운영체제는 해당 프로그램의 코드영역과 데이터 영역을 메모리에 로드하고 

빈 스택과 빈 힙을 만들어 공간을 확보한다.

이제 해당 프로세스를 관리하기 위한 PCB 를 만들어서 값을 초기화 해준다.

지금 설명한 프로세스 생성 과정은 운영체제가 부팅되고 0번 프로세스가 생성될 때 딱 한번 실행된다.

이제 나머지 모든 프로세스는 새로 생성하지 않고 0번 프로세스를 복사해서 쓰게된다.

복사는 fork() 함수를 사용한다.

복사를 이용해서 프로세스를 생성하는 이유는 새로 생성하는 것보다 복사를 하는게 더 빠르기 때문이다.

0번 프로세스를 복사해서 생성되는 프로세스는 자식 프로세스라고 하고 이 자식 프로세스 입장에서 0번 프로세스는 부모 프로세스가 된다.

자식 프로세스는 부모 프로세스의 코드영역, 데이터영역, 스택영역과 PCB 내용 전부를 복사한다.


하지만, 0번 프로세스의 코드와 데이터를 모두 복사해서 실행하면 0번프로세스가 똑같이 실행되는게 아닌가 생각이 들수 있다.

맞는 말이다. 0번 프로세스를 그대로 복사하니 당연한 결과이다.

그럼 자신이 원하는 코드는 어떤식으로 실행시킬 수 있을까?

바로 exec() 함수를 이용하는 것이다.

fork() 함수로 프로세스를 복사한 뒤에 exec() 함수로 실행시키면 

부모를 복사한 자식 프로세스의 코드와 데이터 영역을 원하는 값으로 덮어쓸수 있게 된다.

그럼 이때부터 자식프로세스는 부모 프로세스와 완전 다르게 동작하게 된다.




부모 프로세스와 자식 프로세스는 CPU 스케줄링에 따라 실행되는데,

어떤 프로세스가 먼저 실행될지는 운영체제의 결정에 따른다.

부모 프로세스가 먼저 실행되었다고 가정해보자.

wait() 함수를 호출한다. 해당 함수는 자식 프로세스에게서 exit() 신호가 올때까지 기다리는 시스템 함수이다.

컨텍스트 스위칭을 거쳐 부모프로세스에게 CPU가 할당되어도 

자식 프로세스의 exit() 신호가 오기 전까지는 다른 코드를 실행하지 않는다.

CPU 스케줄링에 의해서 자식 프로세스가 실행된 상황이라고 가정하자.

자식 프로세스는 부모프로세스를 fork() 한다.

그다음 exec() 함수를 사용하여 실행하고자 하는 프로그램을 실행한다.

그럼 해당 프로그램에서 코드와 데이터 영역을 가져와 자식 프로세스를 덮어쓰게 된다.

이제부터 부모 프로세스와는 완전히 다른 자식 프로세스로서 동작하게 된다.

해당 프로그램을 사용하다가 종료가 되면, exit() 를 호출하고 자식프로세스의 종료를 부모 프로세스에게 알린다.

이제 CPU 스케줄링으로 다시 부모 프로세스가 실행된 상황이라고 하자.

wait() 함수로 자식 프로세스의 종료를 기다리고 있었는데, 자식 프로세스로부터 exit() 신호가 왔으므로 

부모 프로세스는 자식 프로세스를 완전히 종료시킨다.





exit() 함수는 자식 프로세스가 부모프로세스에게 정상종료를 알리는 함수이다.

부모프로세스는 자식 프로세스의 Exit Status 를 읽고 자식 프로세스를 정리한다.

만약 부모 프로세스가 자식 프로세스보다 먼저 종료되거나,

자식 프로세스가 비정상적으로 종료돼 exit() 신호를 주지 못해서 부모 프로세스가 Exit Status 를 읽지 못해 

메모리에 계속 살아있는 상태를 "좀비 프로세스" 라고 부른다.

컴퓨터를 오래 켜두면 느려지는 현상이 발생하고는 하는데,

이는 여러 프로세스가 메모리에 올라온 것도 있지만, 좀비 프로세스가 많아져서 메모리를 차지하는 경우가 있다.

컴퓨터를 재부팅하면 메모리가 초기화 되기 때문에 다시 빨라지게 된다.





[쓰레드]

운영체제가 작업을 처리하는 단위는 프로세스이다.

사용자가 운영체제에게 작업을 요구하면 그 만큼 프로세스의 수가 늘어난다.

프로세스를 생성하면 PCB가 생성되고 메모리에 코드, 데이터, 스택, 힙 영역을 만들어줘야 한다.

프로세스의 수가 많아지면, 프로세스의 수 만큼 PCB, 코드, 데이터, 스택, 힙 영역도 만들어줘야 하기 때문에, 너무 무거워진다.


예를들어 웹 브라우저를 실행시키면 프로세스 1개가 생성된다.

여기서 웹브라우저의 탭을 1개 더 추가하면, 기존 프로세스를 복사해서 총 2개의 프로세스가 존재하게 된다.

만약 탭을 10개를 추가하게 된다면, 프로세스의 복사가 10번 일어나고 PCB, 코드, 데이터, 스택, 힙 영역 도 10개가 생성된다.

결국 웹 브라우저가 메모리를 너무 많이 차지하게 된다.

이 웹브라우저의 탭들은 서로 통신을 하려면 IPC(Inter Process Communication) 을 이용해야 하는데

IPC는 통신의 비용이 상대적으로 많이든다.

시스템 개발자들은 어떻게 이러한 문제를 해결할까 고민을 하였고 쓰레드라는 것을 고안하였다.

쓰레드는 프로세스 내에 존재하는 것으로 1개 이상이 존재할 수 있다.

1개의 프로세스의 1개의 스레드가 있을 수 있고, 10개의 쓰레드가 있을 수도 있다.

한 프로세스 내에 쓰레드들은 그 프로세스의 PCB, 코드, 데이터, 힙 영역을 공유한다.

스택은 공유하지 않고, 각 스레드마다 하나씩 가지고 있다.

이제 프로세스내에 여러개의 스레드가 있으니 각각의 스레드도 구분해야 할 필요가 생긴다.

그래서 스레드 id 도 부여하고 이 스레드를 관리하기 위한 Thread Control Block(TCB) 가 생겼다.

이제 운영체제의 관점에서 쓰레드도 구분이 가능해졌다.

이제 운영체자가 작업을 처리하는 단위는 프로세스내에 쓰레드이다.


이전 웹 브라우저 예시를 다시 들어보자.

웹 브라우저를 실행하면 프로세스 하나가 생성되고 스레드도 하나 생성된다.

여기서 탭을 하나 더 추가하면 프로세스를 복사해서 쓰는것이 아니라 쓰레드를 하나 더 생성한다.

이렇게 되면 지금 프로세스내에 스레드가 2개가 있는 것이다.

탭을 10개까지 생성하면 프로세스 1개내에 스레드 10개가 생성된다.



실제 웹브라우저로 설명해보자.

구글의 크롬부라우저는 탭 1개에 1개의 프로세스가 생성된다.

만약 50개의 탭을 열면 50개의 프로세스가 생성되고 50개의 PCB, 코드, 데이터, 스택, 힙 영역이 생성된다.

반면 파이어폭스 브라우저는 처음 4개의 탭에서만 프로세스가 생성되고 추가적인 탭은 프로세스 내에 스레드를 추가하는 방식으로 동작한다.

100개의 탭을 열면 4개의 프로세스가 생성되고 이 4개의 프로세스 내에 100개의 스레드가 존재한다.

이 스레드는 프로세스의 코드 데이터 힙 영역을 공유하니 메모리가 많이 절약된다는 장점이 있다.



마지막으로 프로세스와 스레드의 장단점을 알아보자.

첫번쨰는 안정성이다.

프로세스는 서로 독립적이기 때문에, 하나의 프로세스가 문제가 있더라도 다른 프로세스는 영향을 받지 않는다.

반면 스레드는 하나의 프로세스내에 존재하기 때문에 해당 프로세스에 문제가 생기면 그 안에 있는 모든 스레드에 문제가 발생한다.

이러한 이유로 안정성 측면에서는 프로세스 방식이 스레드 방식보다 더 우수하다고 볼 수 있다.


두번쨰는 속도와 자원이다.

각각의 프로세스는 서로 고유한 자원을 가지고 있다.

코드, 데이터, 스택, 힙 영역을 전부 따로 두고 있고

프로세스간의 통신을 하려면 IPC를 이용해야해서 오버헤드가 크고 속도가 느리다.

반면 스레드는 한 프로세스내에서 스택 영역을 제외한 모든 영역을 공유하기 때문에 오버헤드가 굉장히 작다.

스레드간의 통신을 통해 데이터를 서로 공유할 수 있으니 쉽게할 수 있지만, 공유되는 공간에서 문제가 생길 순 있다. (데이터 동기화 문제)




[다중큐]

프로세스가 생성되면 준비상태로 전환되고 준비상태에서 CPU를 기다리고 있는 프로세스들은 

CPU 스케줄러에 의해 실행상태로 전환된다.

실행상태에 있는 프로세스는 CPU 할당시간이 다 되었다면, 다시 준비상태로 전환되고 i/o 요청이 있다면 대기상태로 작업이 끝났다면 완료상태로 전환된다.

여기서 프로세스가 대기하고 있는 준비상태와 대기상태는 Queue 로 관리된다.

프로세스가 실행상태에서 준비상태로 돌아갈 때, 운영체제는 해당 프로세스의 우선순위를 보고 그에 맞는 준비 큐에 넣는다.

CPU 스케줄러는 "준비상태의 다중큐" 에 들어있는 프로세스들 중에 적당한 프로세스를 선택해서 실행상태로 전환시킨다.

프로세스가 실행상태에서 i/o 요청을 받아 대기상태로 오게되면 i/o 작업 종류에 따라서 분류된 큐에 들어가게 된다.

예를들면 하드디스크 작업은 HDD Queue 에 들어가고 하드디스크 작업이 완료되어 인터럽트가 발생되면 HDD Queue를 뒤져서 다시 꺼내간다.

지금까지 뷰에 프로세스가 들어간다고 했는데 정확히는 프로세스릐 정보를 가지고 있는 pcb 가 들어가게 된다.

프로세스의 정보를 가지고 있는 PCB 는 준비상태의 다중큐에 들어가서 실행되기를 기다리고 있고 

CPU 스케줄러에 의해 실행상태로 전환된다.

이때 CPU 스케줄러는 준비상태의 다중큐를 참조해서 어떤 프로세스를 실행시킬지 결정한다.

I/O 작업도 비슷하다.

실행중인 프로세스에서 I/O 작업이 발생하면 해당 I/O 작업의 종류별로 나뉜 큐에 들어가고 CPU 스케줄러는 이를 참조하여 스케줄링을 한다.






[스케줄링 목표]

스케줄링의 목표에는 여러가지가 있다.


1) 리소스 사용률

CPU 사용률을 높이는 것을 목표로 할 수도 있고

I/O 디바이스의 사용률을 높이는 것을 목표로 할 수도 있다.


2) 오버헤드 최소화

스케줄링을 하기 위한 계산이 너무 복잡하거나 컨텍스트 스위칭을 너무 자주하면 

배보다 배꼽이 더 커지는 상황이 온다.

스케줄러는 이러한 오버헤드를 최소화 하는 것을 목표로 한다.


3) 공평성

모든 프로세스에게 공평하게 CPU 가 할당되어야 한다.

특정 프로세스에게만 CPU 가 계속 할당된다면 불공평한 스케줄러가 된다.

여기서 "공평" 의 의미는 시스템에 따라 달라질 수 있다.

자율주행 자동차에 사용되는 운영체제를 생각해 본다면,

안전의 이유로 장애물을 인지하고 피하는 프로세스가 가장 중요하다.

해당 프로세스에 비해서 음악을 재생하거나 실내 온도를 체크하는 기능은 상대적으로 덜 중요하다.

이 운영체제에서는 장애물을 인지하고 피하는 프로세스에게 CPU 가 많이 할당되는 것이 공평하다.

이와 같은 특수한 경우가 아니고 대부분의 운영체제에서는 모든 프로세스에게 CPU 가 골고루 할당되는게 공평하다고 볼 수 있다.


4) 처리량

같은 시간내에 더 많은 처리를 할 수 있는 방법을 목표로 스케줄링 해야한다.


5) 대기시간

작업을 요청하고 실제 작업이 이루어지기 전까지 

대기하는 시간이 최대한 짧은것을 목표로 한다.


6) 응답시간

대화형 시스템에서 사용자의 요청이 얼마나 빨리 반응하는지가 중요하기 때문에,

응답시간이 짧은것을 목표로 한다.



위의 모든 요소들을 모두 최고의 수준으로 유지하기는 어렵다.

그 이유는 목표간에 서로 상반되는 상황이 있기 때문이다.

예를들어 처리량을 높이기 위해서는 하나의 프로세스에 CPU 를 오래 할당해야 한다.

반면 응답시간을 줄이기 위해서는 여러 프로세스에 골고루 CPU를 할당해야 하는데 서로 상반되기 때문에 

처리량과 응답시간의 목표를 같이 달성할 수 없다.

이때는 사용자가 사용하는 시스템에 따라서 목표를 다르게 설정한다.

터치스크린과 같이 사용자에게 빠른 응답이 필요한 경우는 응답시간이 짧도록 초점을 맞춰야 하고 

과학 계산같은 경우는 처리량이 높도록 초점을 맞춰야 한다.

일반 사용자의 경우는 특별한 목적이 없다면 

위의 6가지 요소가 어느 한쪽에 치우치지 않도록 밸런스를 유지하는게 중요하다.





[FIFO(First In First Out)]

운영체제의 설계자들이 생각한 스케줄링은 처음부터 대단하지 않았다.

우리가 쉽게 생각할 수 있는 것과 크게 다르지 않았다.

FIFO 가 대표적인데 먼저 들어온 작업이 먼저 나간다는 뜻으로 

스케줄링 큐에 들어온 순서대로 CPU를 할당받는 방식이다.

이 방식은 먼저 들어온 프로세스가 완전히 끝나야만 다음 프로세스가 실행될 수 있다.

FIFO 알고리즘의 장점은 단순하고 직관적이라는 것이다.

반대로 단점은 한 프로세스가 완전히 끝나야 다음 프로세스가 시작되기 때문에 

실행시간이 짧고 늦게 도착한 프로세스가 실행시간이 길고 빨리 도착한 프로세스의 작업을 기다려야 한다는 것이다.

또한 I/O 작업이 있다고 한다면, CPU 는 I/O 작업이 끝날때까지 쉬고 있기 때문에, CPU 사용률이 떨어지게 된다.



스케줄링의 성능은 "평균 대기 시간"으로 평가한다.

평균대기시간은 프로세스 여러개가 실행될 때 

이 프로세스들 모두가 실행되기까지 대기시간의 평균을 말한다.

예를들어 프로세스1의 작업시간, 즉 Burst time 이 25초라고 해보자.

프로세스2의 Burst time은 5초 프로세스3의 Burst time 은 4초이다.

프로세스1이 먼저 도착했기 때문에 먼저 실행된다.

프로세스1은 기다림없이 바로 실행되기 때문에 대기시간이 0초이다.

프로세스1이 25초동안 실행을 하고 바로 프로세스2가 실행된다.

프로세스2는 프로세스1의 작업시간만큼(25초) 를 기다렸으니 대기시간이 25초이다.

프로세스2가 5초동안 실행하고 프로세스3이 실행된다.

프로세스3은 프로세스1의 작업시간과(25초) 프로세스2의 작업시간(5초) 만큼을 기다렸기 때문에 

대기시간이 30초이다.

이제 세 프로세스의 평균 대기시간을 구하면 55(0+25+30) / 3 을 해서 18.3초가 된다.

이번에는 순서를 바꾸어 평균 대기시간을 구해보자.

이번에는 Burst time이 짧은 프로세스 순으로 실행해보자.

가장 먼저 프로세스3이 실행된다.

프로세스3은 바로 실행되었기 때문에 대기 시간이 0초이다.

프로세스3이 4초간 실행되고 바로 프로세스2가 실행된다.

프로세스2는 프로세스3의 작업시간만큼(4초) 기다렸기 때문에 대기시간이 4초이다.

프로세스2가 5초동안 실행되고 프로세스1이 실행된다.

프로세스1은 프로세스3의 작업시간만큼(4초)과 프로세스2의 작업시간만큼(5초) 기다렸기 때문에 

대기시간이 9초이다.

이제 세 프로세스의 평균 대기 시간을 구하면 

13(0+4+9) / 3 을해서 4.3 초이다.

프로세스의 실행순서만 바꿧는데 평균 대기시간의 차이가 많이 나는 모습을 볼 수 있다.

FIFO 알고리즘은 프로세스의 Burst Time 에 따라 성능의 차이가 심하게 나기 때문에 현대 운영체제에서는 잘 쓰이지 않고 일괄처리 시스템에 쓰인다.



[SJF (Shortest Job First)]

FIFO 에서 Burst Time이 짧은 프로세스를 먼저 실행했을 때 평균 대기시간이 짧아진다.

그럼 Burst Time 이 짧은 프로세스를 먼저 실행하는 알고리즘을 만들기로 하고 SJF(Shortest Job First)라는 이름을 붙였다.

한글로 해석하면 "짧은 작업 먼저" 라는 뜻이 된다.

SJF 는 이론적으로 FIFO 보다 성능이 더 좋다.

하지만, 실제로 구현하려고 하니 문제가 발생하였다.

문제는 크게 두가지가 존재한다.

첫번째 문제는 어떤 프로세스가 얼마나 실행될지 예측하기 힘들다는 것이다.

예를들어 사용자가 인터넷 브라우저를 실행시키고 뮤직플레이어를 실행시켰다고 가정해보자.

노래를 틀어놓고 브라우저에서 날씨만 확인하고 브라우저는 바로 종료할 수도 있고, 인터넷 쇼핑을 하느라 계속 켜놓을 수도 있다.

이처럼 프로세스의 종료시간을 예측하기란 거의 불가능에 가깝다.

두번째 문제는 Burst time이 긴 프로세스는 아주 오랫동안 실행되지 않을 수도 있다는 것이다.

SJF는 Burst time이 짧은 프로세스가 먼저 실행되기 때문에 Burst time이 긴 프로세스는 뒤로 밀려난다.

만약 Burst time이 짧은 프로세스가 중간에 계속 들어온다면 Burst time이 긴 프로세스는 앞에 모든 프로세스가 종료되기 까지 기다려야 하기 때문에 굉장히 불공평하게 느껴질 수 있다.

이러한 문제점 때문에 SJF 알고리즘은 사용되지 않는다.




[RR(Round Robin)]

가장 단순한 FIFO 알고리즘에서부터 단점을 해결해보기로 하였다.

FIFO 알고리즘의 단점은 먼저들어온 프로세스가 전부 끝나야 다음 프로세스가 실행되는 것이다.

이 문제를 해결하기 위해서 한 프로세스에게 일정 시간만큼 CPU를 할당하고 할당시간이 지나면 강제로 다른 프로세스에게 일정 시간만큼 CPU를 할당한다.

강제로 CPU 를 뺏긴 프로세스는 큐의 가장 뒷부분으로 밀려난다.

이 알고리즘을 Round Robin 알고리즘이라고 부른다.

프로세스에게 할당하는 일정 시간은 "타임슬라이스" 또는 "타임 퀀텀" 이라고 부른다.

그럼 RR 의 성능을 살펴보자.

타임 슬라이스가 10초인 시스템이라고 가정하고 평균 대기시간을 계산해보자.

프로세스1을 p1, 프로세스2를 p2, 프로세스3을 p3 이라고 해보자.

p1 의 Burst time은 25초, p2의 Burst time은 4초, p3의 Burst time은 10초라고 해보자.

이 프로세스들은 동시에 큐애 들어왔고 실행순서를 P1, P2, P3 라고 가정해보자.

P1은 큐에 들어오자마자 실행되기 때문에 대기시간이 0초이다.

P1은 타임슬라이스 10초만큼 실행하다가 시간을 초과했기 때문에 p2에게 CPU 를 뺏기고 P1의 남은 작업은 가장 뒤로 이동한다.

p2는 p2이 실행하는 10초를 기다렸기 때문에 대기시간은 10초가 된다.

p2는 Burst time이 4초이기 때문에, 4초가 지나면 p3에게 cpu를 양보하고 작업을 마친다.

p3는 p1과 p2의 실행이 완료될 때까지 14초를 기다렸다. p3의 대기시간은 14초이다.

p3는 Burst time인 10초 동안 실행하고 작업을 마친다.

다음으로 p1이 다시 실행되는데 p1 은 p2와 p3 의 실행이 완료될때까지 14초를 기다렸다.

p1의 대기시간은 14초이다.

현재 p1은 Burst time이 15초로 타임슬라이스보다 크기 떄문에 타임슬라이스 값인 10초만 실행한다.

그리고 Burst time이 5초인 p1이 큐의 마지막으로 이동하는데 

대기중인 프로세스가 없기 때문에 바로 p1이 실행된다. (대기시간 0)

이제 이 대기 시간으로 평균 대기 시간을 구해보자.

p1의 첫번째 대기시간은0초

p2의 대기시간 10초 

p3 의 대기시간 14초

p1의 두번째 대기시간 14초

p1의 세번째 대기시간 0초를 더하면 38초이고 이를 프로세스 갯수인 3으로 나누면 평균 대기시간이 12.67 초로 나온다. (FIFO 알고리즘은 18초가 나온다.)

다른 상황에서는 FIFO 알고리즘과 RR 알고리즘의 대기시간이 비슷할수도 있다.

평균대기시간이 비슷하다면, RR 알고리즘이 더 비효율적인 방식이다.

RR은 컨텍스트 스위칭이 있기 때문에 컨텍스트 스위칭 시간이 더 추가되기 때문이다.


RR 알고리즘의 성능은 타임 슬라이스 값에 따라 크게 달라진다. 두가지 상황으로 알아보자.

먼저 타임슬라이스가 큰 경우부터 살펴보자.

이론적으로 타임슬라이스가 무한대라고 가정하면 먼저 들어온 프로세스의 작업이 종료될 때까지 실행하니까 FIFO 알고리즘이 되어버린다.

타임 슬라이스가 5초인 시스템에서 웹브라우저와 뮤직 플레이어를 실행시키면, 웹 브라우저가 5초 동작하다가 멈추고 음악이 5초 재생되다가 멈추고 다시 웹 브라우저가 5초 동작하니 굉장히 끊기게 될 것이다.


이번에는 타임슬라이스가 작은 경우를 살펴보자.

타임 슬라이스를 1밀리초로 아주 작은 값으로 설정하면 웹 브라우저와 뮤직 플레이어가 동시에 동작하는 것 처럼 느껴진다.

하지만, 타임 슬라이스를 이렇게 너무 작게 설정해버리면 컨텍스트 스위칭이 너무 자주 일어나게 되고 타임 슬라이스에서 실행되는 프로세스의 처리량보다 컨텍스트 스위칭을 처리하는 양이 훨씬 커져서 

배보다 배꼽이 더 커지는 상황이 발생한다. 이러한 상황을 "오버헤드가 너무 크다" 라고 표현한다.

최적의 타임슬라이스를 결정하는 방법은 사용자가 느끼기에 여러 프로세스가 버벅거리지 않고 동시에 실행되는 것처럼 느껴지면서 오버헤드가 너무 크지 않는 값을 찾아야 하는 것이다.

실제로 Windows Os 는 타임슬라이스가 20ms, Unix 는 100ms 로 굉장히 짧다.



[MLFQ (Multi Level Feedback Queue)]

MLFQ 는 오늘날 운영체제에서 가장 일반적으로 쓰이는 CPU 스케줄링 방법이다.

MLFQ는 RR의 업그레이드된 알고리즘이기 때문에 RR의 예시로 알아보자.

프로세스 두개가 있다. 첫번째 프로세스는 P1이고 두번째 프로세스는 P2이다.

P1는 I/O 작업 없이 CPU 연산만 하는 프로세스이다.

P2는 1초 CPU 연산을 하고 10초동안 I/O 작업을 수행한다.

여기서 P1은 대부분의 시간을 CPU 연산을 하기 때문에 CPU Bound Process 라고 한다.

반면, P2는 대부분의 시간을 I/O 작업으로 보내고 CPU연산은 조금 하기 때문에 I/O Bound Process 라고 한다.

CPU Bound Process가 가장 중요하게 생각하는 건 CPU의 사용률과 처리량이다.

반면 I/O Bound Process 가 가장 중요하게 생각하는건 응답속도이다. 키보드나 마우스 입력을 했는데 반응이 늦으면 안되기 떄문이다.

이제 위의 P1, P2 프로세스 두 개를 가지고 두 가지 상황에서 실행시켜 보자.

첫번째 상황은 타임 슬라이스가 100초인 경우이다.

P2가 먼저 실행된다고 가정해보자.

P2는 1초 실행되고 I/O 작업을 요청을 하고 기다린다.

이제 P1이 실행되는데 타임 슬라이스 크기인 100초만큼 실행된다.

P1이 실행되는 도중에 10초가 지났을 때 P2가 요청한 I/O 작업이 완료되고 인터럽트가 발생된다.

그럼 P2는 다시 큐에 들어가서 CPU를 할당받을 준비를 한다.

P1는 100초 실행되면 CPU를 뺏기게 되고 큐에 있던 P2는 다시 1초 실행되고 I/O 작업을 요청하고 다시 기다린다.

위의 작업이 모든 작업이 완료될 때까지 계속 반복된다.



두번째 상황을 살펴보자.

두번째 상황은 타임슬라이스가 1초인 경우이다.

P2가 1초 실행되고 I/O 작업을 요청하고 기다린다.

이제 P1이 실행되는데 타임 슬라이스 크기인 1초만큼 실행된다.

지금 P2는 I/O 작업이 끝나지 않아서 계속 기다리는 상황이다. 그렇기 때문에 P1은 종료되고 바로 큐에 들어가는데

큐가 비어있기 때문에 P1이 다시 실행된다. 이렇게 P1이 10번 즉 10촣 동안 실행되면 P2의 I/O 작업이 끝나 인터럽트가 발생되고 

P2는 큐에 들어간다. 그럼 P2는 다시 1초 실행되고 다시 I/O 작업을 요청하고 기다린다.

위와같은 방식으로 모든 작업이 완료될 때 까지 계속 반복된다.


첫번째 상황을 보면 CPU 사용률을 보면 CPU는 쉬지않고 일하기 때문에 CPU 사용률이 100%이다.

하지만 I/O 사용률을 보면 P1이 실행되는 동안 P2의 I/O 작업이 완료된 시점부터 기다리는 시간이 발생하기 때문에 101 분의 10으로 대략 10% 밖에 되지 않는다.

두번째 상황도 CPU 사용률을 보면 CPU 는 쉬지 않고 일하기 때문에 100% 이다.

하지만, I/O 사용률은 첫 번째 상황과 조금 다르다.

P1의 타임슬라이스가 작기 때문에 P2가 첫 번째 상황처럼 기다리며 낭비되는 시간이 거의 없고 11 / 10 으로 대략 90% 정도의 사용률이 나오게 된다.

위의 결과를 보면 CPU 사용률와 I/O 사용률을 보면 타임슬라이스가 작은 값이 더 성능이 좋다는 결론이 나온다.


타임슬라이스가 100초에서 1초로 작아질 때 P1, P2 입장에서 보면 한쪽은 손해를 보고 한쪽은 이득을 보는 구조이다.

CPU Bound Process인 P1은 100초였던 실행시간이 1초로 줄었지만 연속으로 실행되기 때문에 손해가 없는 것처럼 보여진다.

하지만, 컨텍스트 스위칭을 하기 때문에 오버헤드가 생겨서 손해를 보게된다.

반면 I/O Bound Process 인 P2는 I/O 사용률이 굉장히 높아졌기 때문에 이득을 봤다.

운영체제를 연구하는 사람들은 손해보는 프로세스가 어떻게 하면 손해보지 않을까 고민을 하게 된다.

바로 여기서 MLFQ가 탄생한다.

MLFQ는 기본적으로 CPU 사용률과 I/O 사용률이 좋게 나오는 작은 크기의 타임슬라이스를 선택한다.

그리고 P1 과 같은 CPU Bound Process들에게는 타임 슬라이스를 크게 주는 것이다.

그럼 운영체제 입장에서 CPU Bound Process 와 I/O Bound Process를 어떻게 구분할까?

CPU를 사용하는 프로세스가 실행하다가 스스로 CPU를 반납하면 CPU 사용이 적은거니 I/O Bound Process일 확률이 높다.

반대로 CPU 를 사용하는 프로세스가 타임 슬라이스 크기를 오버해서 CPU 스케줄러에 의해 강제로 CPU를 뺏기는 상황이면 

CPU 사용이 많은 것이니 CPU Bound Process 일 확률이 높다.

이러한 아이디어를 통해서 우선순위를 가진 큐를 여러개 준비해둔다.

우선순위가 높으면 타임 슬라이스가 작고 우선순위가 낮을 수록 타임슬라이스의 크기가 커진다.

만약 P1 처럼 타임 슬라이스 크기를 오버해서 강제로 CPU 를 뺏긴다면 P1은 원래 있던 큐보다 우선순위가 더 낮은 큐로 이동한다.

그러면 다음번에 실행될 때는 타임 슬라이스 크기가 조금 더 커지게 되고 여기서도 부족하다면 다음엔 더 큰 타임 슬라이스를 할당받게 된다. (낮은 우선순위 큐로 이동함)

최종적으로는 타임 슬라이스가 무한초에 가깝게 할당되기 때문에 FIFO 처럼 연속적으로 작업을 마칠 수 있게 된다.



[프로세스 간 통신]

프로세스는 독립적으로 실행되기도 하지만 다른 프로세스와 데이터를 주고 받으며 통신을 하는 경우도 있다.

통신은 한 컴퓨터 내에서 실행되고 있는 다른 프로세스와 할 수도 있고 네트워크로 연결된 다른 컴퓨터에 있는 프로세스와 할 수도 있다.

그럼 프로세스 간 통신의 종류를 알아보자.

첫번째는 한 컴퓨터 내에서 프로세스 간 통신을 하는 방법으로 파일과 파이프를 이용하는 방법이다.

먼저 파일을 이용하는 방법은 통신하려는 프로세스들이 하나의 파일을 이용해 읽고 쓰는 방법이다.

파이프를 이요이하는 방법은 운영체제가 생성한 파이프를 이용해 데이터를 읽고 쓰는 방법이다.


두번째로는 스레드를 이용한 방법이 있다.

이 방법은 한 프로세스 내에서 쓰레드 간 통신을 하는 방법이다.

스레드는 코드, 데이터, 힙 영역을 공유하고 스택만 각자 자기의 것을 가지고 있다.

여기서 데이터 영역에 있는 전역변수나 힙을 이용하면 통신이 가능하다.


세번째 방법은 네트워크를 이용한 방법이다.

운영체제가 제공하는 소켓통신이나 다른컴퓨터에 있는 함수를 호출하는 RPC(원격 프로시저 호출)를 이용해 통신하는 방법이 있다.



[공유자원과 임계구역]

프로세스 간 통신을 할 때 공동으로 이용하는 변수나 파일들이 있는데 이것들을 "공유자원" 이라고 한다.

공유자원은 여러 프로세스가 공유하고 있기 때문에 각 프로세스의 접근 순서에 따라 결과가 달라질 수 있다.

또한, 컨텍스트 스위칭으로 시분할 처리를 하기 때문에 어떤 프로세스가 먼저 실행되고 어떤 프로세스가 나중에 실행되는지 예측하기가 힘들다.

따라서 연산 결과를 예측하기가 힘들고 여기서 발생한 문제를 "동기화 문제" 라고 부른다.

여러 프로세스가 동시에 사용하면 안되는 영역을 정의했는데 이를 "임계 구역(Critical Section)" 이라고 부른다.

공유자원을 서로 사용하기 위해 결쟁하는 것은 경쟁조건(Race Condition) 이라고 부른다.

임계 구역에서 발생하는 문제를 해결하기 위해서는 상호 배제(Mutual Exclusion) 메커니즘이 필요하다.

상호 배제 메커니즘의 요구사항은 세가지가 있다.

1) 임계영역엔 동시에 하나의 프로세스만 접근한다.

2) 여러 요청에도 하나의 프로세스의 접근만 허용한다.

3) 임계구역에 들어간 프로세스는 빠르게 나와야 한다.




[세마포어]

상호배제 메커니즘의 한가지인 세마포어에 대해서 알아보자.

회사에서 직원들이 문서 작업을 한다.

직원들은 자기 문서의 작업을 마치면 프린터로 작업 결과물을 출력한다.

프린터는 네트워크를 통해 여러 컴퓨터에 연결되어 있다고 가정해보자.

여기서 공유자원은 "프린터" 가 된다.

직원 A 와 직원 B 가 우연히 동시에 프린터 출력을 하게 되었다.

그럼 직원A의 컴퓨터와 직원B의 컴퓨터가 프린터를 차지하기 위해서 경쟁조건(Race condition) 이 되어버린다.

이러면 프린트의 결과물이 직원A의 결과물과 직원B의 결과물이 섞여서 나오기 때문에 쓸모가 없어진다.

이런 상황을 막기위해서 팀장님이 아이디어를 내었다.

프린터실을 따로 만들고 거기에 컴퓨터와 프린터를 설치해둔다.

그리고 밖에는 이 방의 열쇠를 관리하는 직원을 따로 둔다.

이제 프린터를 사용하고 싶은 직원A는 관리자에게 열쇠를 받아서 프린터실로 입장해 프린터를 혼자 사용할 수 있다.

조금 늦게 온 직원B 도 프린터를 사용하고 싶어서 프린터실로 들어가려고 했지만 열쇠관리자는 이미 열쇠를 직원A에게 준 상태라 직원B 에게는 잠시 대기하고 있으라고 말한다.

직원A가 프린터를 다 사용하면 방에서 나오고 열쇠 관리자에게 열쇠를 반납한다.

그럼 이제 직원B가 열쇠를 받고 프린터실로 들어가 프린터를 사용할 수 있다.

위의 예시를 들어 설명한것이 세마포어 메커니즘이다.

단순하고 무식해보이지만 동기화에서 가장 중요한 개념이다.


위의 상황을 운영체제에서 쓰이는 용어로 매칭시켜보자.

프린터를 사용하는 직원들은 "프로세스"이다.

프린터는 여러 프로세스들이 같이 쓰고 있는 "공유 자원" 이다.

프린터를 쓰기 위해 프로세스가 기다리는 해당 공간은 "대기큐" 이다.

열쇠관리자는 "운영체제"이다.

마지막으로 열쇠 관리자가 가지고 있는 열쇠를 "세마포어" 라고 한다.

세마포어는 실제로 정수형 변수이다.



이제 세마포어를 코드로 어떤식으로 쓰는지 알아보자.
--- 예시 ---- 



예시에서는 열쇠가 하나인 세마포어를 설명했다. 열쇠가 하나인 세마포어를 이진 세마포어 라고 부르고 뮤텍스로 부르기도 한다.

세마포어는 실제로 여러개의 열쇠를 가질 수 있다.

세마포어가 정수형 변수라고 설명했는데, 공유자원이 두개라면 세마포어 값은 2 즉 열쇠 두개로 설정해주면 된다.

이렇게 보면 세마포어가 굉장히 좋은 동기화 방법인 것처럼 보이지만, 단점이 존재한다.


1)
wait(s);
임계 구역
wait(s);

2)
signal(s);
임계 구역
signal(s);

3)
signal(s);
임계구역
wiat(s);

wait()함수와 signal() 함수의 순서를 이상하게 호출해서 세마포어를 잘못 사용할 가능성이 있다는 것이다.




[모니터]

모니터는 세마포어의 단점을 해결한 상호배제 메커니즘이다.

모니터는 따로 운영체제가 처리하는 것이 아닌, 프로그래밍 언어 차원에서 지원하는 방법이다.

대표적으로 자바에서 모니터를 지원하는데 

"synchronized" 라는 키워드가 붙는다.

자바에서 "synchronized" 라는 키워드가 붙으면 이 키워드가 붙은 함수들은 동시에 여러 프로세스를 실행시킬 수 없다.

즉, 상호배제가 완벽하게 이루어진다.

모니터의 구현만 완벽하다면 프로그래머는 세마포어처럼 wait()함수나 signal() 함수를 임계영역에 감싸지 않아도 돼서

편리하고 안전하게 코드를 작성할 수 있다.



















































